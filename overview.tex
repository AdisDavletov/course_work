\chapter{Обзор методов обучения}

\section{Наивный байесовский классификатор}
Наивный байесовский классификатор --- классификатор, основанный на теореме Байеса и "<наивном"> предполжении о том, что события представляющие объект классификации независимы. Объекты классификации в данной модели, как можно было понять из вышесказанного, представляются в виде последовательности некоторых независимых событий. Классификация осуществляется выбором того класса, у которой максимальна условная вероятность события $ c_i $ при условии выполнения событий, описывающих указанный объект \cite{irinaRish:naiveBayes}.

$$
P(c_i|o) = \frac{P(o | c_i)P(c_i)}{P(o)} =
\frac{P(x_1, \ldots , x_n|c_i)P(c_i)}{P(x_1, \ldots , x_n)} =
\frac{P(x_1|c_i)\ldots P(x_n|c_i)P(c_i)}{P(x_1) \ldots P(x_n)}
$$
$$
i = \arg\limits_{j}\max P(c_j|o)
$$
В формулах выше введены следующие обозначения: \\
$o$ - это объект классификации, \\ $c_i$ - событие, что объект классификации принадлежит $i$-му классу, \\ $x_1, \ldots, x_n$ - независимые события, представляющие объект классификации и \\ $i$ - результат работы классификатора, класс к которому соотнес классификатор поданый ему объект $o$.


Так как при классификации нам интересен лишь максимум условных вероятностей $ P(c_j | o) $, можно обойтись без вычисления вероятности объекта классификации $P(o)$, т.е. без вычисления вероятностей $P(x_1), \ldots, P(x_2)$

В задачах анализа тональности текста объектом классификации является документ (некоторый текст) и событиями, которые мы "<наивно"> предполагаем независимыми, являются слова появляющиеся в них.

Подходы к подсчету условных вероятностей слов $P(x_k|c_i)$ и вероятностей классов $P(c_i)$ могут быть различными. Например, можно брать значением $P(x_k|c_i)$ нормированную частоту появления слова $x_k$ в документах данного класса $c_i$, а вероятность класса $P(c_i)$ - отношением количества документов класса $c_i$ к общему числу документов в обучающей выборке:
$$
	P(x_k|c_i) = \frac{\# x_k \; in \; c_i}{\# words \; in \; c_i} \quad P(c_i) = \frac{\# documents \; of \; c_i}{\#total \; documents}
$$

Согласно \cite{Narayanan:naiveBayes}, результаты обучения и последующей классфикации на данных из IMDb \cite{imdb:data} на положительные и отрицательные классы с использованием улучшенной версии этого алгоритма дали точность 88.80\%
\section{Логистическая регрессия}
Логистическая регрессия (для бинарной классификации) --- статистическая модель, которая по обучающей выборке пытается подобрать параметры модели $\theta_k$ так, чтобы гиперплоскость, задаваемая ими, разделяла как можно больше данных из одного, заранее определенного класса $c_1$ из обучающей выборки  от данных из второго класса $c_2$ \cite{AndrewNg:logisticRegression}.

Результат модели лежит в вероятностном пространстве.
$$
h(x, \theta) = \frac{1}{1 + \exp(-x^T\theta -\theta_0)}
$$
\\
$x = (x_1, \ldots, x_n)$ - вектор признаков объекта классификации, \\
$\theta = (\theta_1, \ldots, \theta_n)$, $\theta_0$ - параметры модели (веса),
\\
$n$ - количество признаков

Классификатор соотносит объект классификации $x$ к классу $c_1$, при $h(x, \theta) \geq 0.5$ и ко второму классу $c_2$, в противном случае.


\section{NBSVM}
Та же самая логистическая регрессия в задачах двухклассовой классификации с определенным образом формируемым вектором признаков для объекта классификации \cite{nbsvm}. Объектом классификации является текст (мешок слов). Имеется два класса: положительный и отрицательный. Вектор признаков формируется как логарифм отношения частот слова в положительном и отрицательном классах.